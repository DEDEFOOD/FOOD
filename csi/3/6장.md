### 케라스

- 여러 딥러닝 프레임워크 중 하나
- 직관적이고 사용하기 쉽다
- 신경망 모델을 구축할 수 있는 고수준 API라이브러리
- 텐서플로 2.0에 기본 API로 채택

#### 인공신경망

- 두뇌의 신경세포인 뉴런을 수학적으로 모방한 모델
- 인공 신경망에 들어온 입력값이 임계치를 넘어 활성화되면 다음 뉴런으로 출력값을 내보냄

![](6장_assets/2023-04-11-08-50-34-image.png)

- 뉴런 1개에 입력값은 임의 설정 갯수, 출력값은 1개
  
  - 출력값을 늘리려면 뉴런 수 늘리기

- 입력값 x0, x1, x2 > 가중치 w0, w1, w2 , 편향값 b
  y = (w0x0 + w1x1 + w2x2) + b

#### 활성화 함수

- 입력된 신호가 특정한 강도 이상일 때만 다음 뉴런으로 신호 전달
- 3가지

##### 스텝 함수

- 입력값이 0보다 크면 1로, 0이하일 때는 0으로 만든다
- 즉, 입력값이 양수일 때만 활성화
- 결과값 : 0 또는 1 > 합.불 / T.F 등 이진 분류일 때 사용

![](6장_assets/2023-04-11-09-15-12-image.png)

- 결과를 너무 극단적으로 나누기 때문에 실제로 사용하기엔 문제가 있다.
- 0.1의 경우 0에 가깝지만 무조건 1로 출력

##### 시그모이드 함수

- 스텝함수에서 판단 기준이 되는 임계치 부근의 데이터를 고려하기 위해
  완만한 형태로 표현

![](6장_assets/2023-04-11-10-00-38-image.png)

![](6장_assets/2023-04-11-10-00-57-image.png)

- 0에서 1까지의 출력값이 확률로 표현
- 합격일 확률, 거짓일 확률
- 입력값이 0.2면 0.54 확률
- 시그모이드 입력값이 커질수록 미분값이 0으로 수렴하게 되는 단점
  - 가중치와 편향을 조정하는 도구가 미분 > 학습이 잘 안된다
- 분모에 exp함수를 사용 > 연산 비용이 크다

##### ReLU 함수

- 가장 많이 사용

![](6장_assets/2023-04-11-10-06-47-image.png)

![](6장_assets/2023-04-11-10-06-52-image.png)

- 입력값이 0 이상이면 기울기 1인 직선

- 입력값 0보다 작으면 결과값 0

- 연산 비용이 크지 않아 학습속도가 빠르다

- 심층 신경망 : 입력층, 은닉층, 출력층으로 구성

![](6장_assets/2023-04-11-10-08-41-image.png)

- 뉴런과 은닉층을 늘리면 성능이 좋아지지만 학습비용이 올라간다

- 신경망의 순전파
  입력층으로부터 출력층까지 데이터가 순반향 전파
  현 단계 뉴런의 가중치와 전 단계 뉴런의 출력값의 곱을 입력값으로 받음
  이 값은 다시 활성화 함수를 통해 뉴런으로 전파

![](6장_assets/2023-04-11-10-10-18-image.png)

- 신경망의 역전파
  실제값과 비교해 오차가 많이 발생한 경우
  오차가 줄어드는 방향으로 가중치를 역방향으로 갱신해나감

![](6장_assets/2023-04-11-10-41-02-image.png)

![](6장_assets/2023-04-11-17-25-02-image.png)

![](6장_assets/2023-04-11-17-27-40-image.png)

![](6장_assets/2023-04-11-17-30-11-image.png)
