### 양방향 LSTM

- 챗봇 엔진에 개체명 인식을 위해 사용

- 순화 신경망 모델의 일종으로 시퀀스 또는 시계열 데이터의 패턴

### RNN

- 순환 신경망
- 은닉층의 출력값을 출력층과 그 다음 시점의 은닉층의 입력으로 전달해 순환

![](LSTM%20모델_assets/2023-04-18-17-32-58-image.png)

Xt = 현재 시점 입력벡터

Yt = 현재 시점 출력벡터

은닉층 노드는 이전 시점의 상태값을 저장하는 메모리 역할 > 메모리 셀

메모리 셀의 출력 벡터는 출력층과 다음 시점 메모리 셀에전달 : 은닉 상태

ht = 현재 시점 은닉상태

![](LSTM%20모델_assets/2023-04-18-17-35-16-image.png)

완전 연결 계층

    

![](LSTM%20모델_assets/2023-04-18-17-35-59-image.png)

![](LSTM%20모델_assets/2023-04-18-17-36-15-image.png)

![](LSTM%20모델_assets/2023-04-18-17-36-23-image.png)

RNN 은 모든 시점에서 동일한 가중치와 편향값 사용

![](LSTM%20모델_assets/2023-04-18-17-37-17-image.png)![](LSTM%20모델_assets/2023-04-18-17-37-25-image.png)



![](LSTM%20모델_assets/2023-04-18-00-31-53-image.png)

    ![](LSTM%20모델_assets/2023-04-18-17-53-38-image.png)

- RNN은 입력 시퀀스의 시점이 길어질수록 앞쪽의 데이터가 뒤쪽으로 잘 전달되지 않는다

- 다층 구조로 쌓으면 입력과 출력 사이의 연관성이 줄어 장기 의존성 문제

### LSTM

- 셀 상태값

![](LSTM%20모델_assets/2023-04-18-17-55-45-image.png)

- 입력게이트 : 현재 정보를 얼마나 기억할지 결정
  
  - 현재 시점입력값과 이전 시점 은닉 상태값을 2개의 활성화 함수로 계산
  
  - 시그모이드 : 0~1, 하이퍼블릭 탄젠트 : -1~1 > 두 값의 곱
  
  ![](LSTM%20모델_assets/2023-04-18-17-57-21-image.png)

- 삭제게이트 : 이전 시점의 셀 상태값을 삭제하기 위해 사용
  
  - Xt와 ht-1 을 시그모이드로 0~1 사이로 출력
  
  ![](LSTM%20모델_assets/2023-04-18-17-58-28-image.png)

- 출력게이트 : 결과값은 현재 시점의 은닉 상태를 결정
  
  - 장기상태(셀 상태값)  > .영향 > 단기상태(은닉상태)
  
  ![](LSTM%20모델_assets/2023-04-18-17-59-36-image.png)

    ![](LSTM%20모델_assets/2023-04-18-17-59-47-image.png)

    

















![](LSTM%20모델_assets/2023-04-18-00-34-29-image.png)

![](LSTM%20모델_assets/2023-04-18-00-34-49-image.png)

    
